{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a961411-0aa5-4ce6-80b5-d475c6614b40",
   "metadata": {},
   "source": [
    "# Implémentez un modèle de scoring\n",
    "\n",
    "*Notebook modélisation*\n",
    "\n",
    "L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un algorithme de classification en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.).\n",
    "\n",
    "**Mission :**\n",
    "\n",
    "- Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.\n",
    "\n",
    "- Analyser les features qui contribuent le plus au modèle, d’une manière générale (feature importance globale) et au niveau d’un client (feature importance locale), afin, dans un soucis de transparence, de permettre à un chargé d’études de mieux comprendre le score attribué par le modèle.\n",
    "\n",
    "- Mettre en production le modèle de scoring de prédiction à l’aide d’une API et réaliser une interface de test de cette API.\n",
    "\n",
    "- Mettre en œuvre une approche globale MLOps de bout en bout, du tracking des expérimentations à l’analyse en production du data drift.\n",
    "    + Dans le notebook d’entraînement des modèles, générer à l’aide de MLFlow un tracking d'expérimentations\n",
    "    + Lancer l’interface web 'UI MLFlow\" d'affichage des résultats du tracking\n",
    "    + Réaliser avec MLFlow un stockage centralisé des modèles dans un “model registry”\n",
    "    + Tester le serving MLFlow\n",
    "    + Gérer le code avec le logiciel de version Git\n",
    "    + Partager le code sur Github pour assurer une intégration continue\n",
    "    + Utiliser Github Actions pour le déploiement continu et automatisé du code de l’API sur le cloud\n",
    "    + Concevoir des tests unitaires avec Pytest (ou Unittest) et les exécuter de manière automatisée lors du build réalisé par Github Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075208bc-0fc6-4f94-816a-6b785c17de4d",
   "metadata": {},
   "source": [
    "## 1 - Préparer l'environnement d'expérimentation\n",
    "\n",
    "### 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bc62f7-f6f2-4e72-9de2-92187b3730a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version 1.26.4\n",
      "pandas version 2.1.4\n",
      "matplotlib version 3.8.0\n",
      "seaborn version 0.13.2\n",
      "mlflow version 2.20.1\n",
      "sklearn version 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tracking\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "print(\"numpy version\", np.__version__)\n",
    "print(\"pandas version\", pd.__version__)\n",
    "print(\"matplotlib version\", matplotlib.__version__)\n",
    "print(\"seaborn version\", sns.__version__)\n",
    "print(\"mlflow version\", mlflow.__version__)\n",
    "print(\"sklearn version\", sklearn.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee8c9e-e291-43c7-9d00-e9084f5fd3cb",
   "metadata": {},
   "source": [
    "### 1.2 - Lancement du serveur MLFlow\n",
    "\n",
    "Dans le terminal de Powershell Prompt d'Anaconda, lancer cette instruction :\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns\n",
    "\n",
    "Cette commande permet de :\n",
    "- Stocker les métadonnées du Model Registry dans une base de données locale SQLite.\n",
    "- Stocker les artefacts (modèles, fichiers) en local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c481534-9c18-4b31-a0cd-94af8ff0fa9d",
   "metadata": {},
   "source": [
    "### 1.3 - Initialisation du Tracking MLFlow de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e772755d-867c-40db-b092-46c9b31c4620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ignorer Git\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'\n",
    "\n",
    "# Vérification si la variable d'environnement est bien définie\n",
    "print(os.environ.get('GIT_PYTHON_REFRESH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841d7eb5-f0da-41ed-84c6-4329f85440ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to http://127.0.0.1:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=modele_test_quickstart_2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000226E92F2590>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000226E92F2590>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 875 (4 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=modele_test_quickstart_2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000226E92F2590>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:181\u001b[0m, in \u001b[0;36mhttp_request\u001b[1;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[0;32m    182\u001b[0m         method,\n\u001b[0;32m    183\u001b[0m         url,\n\u001b[0;32m    184\u001b[0m         max_retries,\n\u001b[0;32m    185\u001b[0m         backoff_factor,\n\u001b[0;32m    186\u001b[0m         backoff_jitter,\n\u001b[0;32m    187\u001b[0m         retry_codes,\n\u001b[0;32m    188\u001b[0m         raise_on_status,\n\u001b[0;32m    189\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    190\u001b[0m         verify\u001b[38;5;241m=\u001b[39mhost_creds\u001b[38;5;241m.\u001b[39mverify,\n\u001b[0;32m    191\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    192\u001b[0m         respect_retry_after_header\u001b[38;5;241m=\u001b[39mrespect_retry_after_header,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\utils\\request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[1;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method, url, allow_redirects\u001b[38;5;241m=\u001b[39mallow_redirects, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=modele_test_quickstart_2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000226E92F2590>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Démarrer une nouvelle expérimentation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodele_test_quickstart_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[1;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(experiment_name)\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\tracking\\client.py:1293\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m \n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracking_client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:502\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m    495\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mget_experiment_by_name(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:522\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[1;34m(self, experiment_name)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[1;32m--> 522\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_endpoint(GetExperimentByName, req_body)\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[1;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[0;32m     80\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[0;32m     81\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call_endpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:374\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[1;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    373\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[1;32m--> 374\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:204\u001b[0m, in \u001b[0;36mhttp_request\u001b[1;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUrlException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miu\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMlflowException\u001b[0m: API request to http://127.0.0.1:5000/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=modele_test_quickstart_2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000226E92F2590>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))"
     ]
    }
   ],
   "source": [
    "# Définir le serveur de tracking (local ou distant) - ici local\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Démarrer une nouvelle expérimentation\n",
    "mlflow.set_experiment(\"modele_test_quickstart_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f986afe-4c78-46af-8b21-e2a63bffac7d",
   "metadata": {},
   "source": [
    "### 1.4 - Création d'un modèle pour tester l'initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0ccf8-5e7e-4ef7-b943-1878e53ac01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données Iris (pour la démonstration)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Diviser les données en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model hyperparameters\n",
    "params = {\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = LogisticRegression(**params)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train)\n",
    "    \n",
    "# Faire des prédictions sur le jeu de test\n",
    "y_pred = model.predict(X_test)\n",
    "#y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probabilités pour AUC binaire\n",
    "y_pred_prob = model.predict_proba(X_test)  # Probabilités pour AUC\n",
    "\n",
    "# Calculer accuracy et AUC\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# auc = roc_auc_score(y_test, y_pred_prob) --> si le problème était binaire\n",
    "auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46ea0b-376d-4aa7-a8bd-9ac938f29acd",
   "metadata": {},
   "source": [
    "### 1.5 - Enregistrer le modèle et ses métadonnées dans MLFlow\n",
    "\n",
    "Créons une fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12888b4a-82c5-4ee6-9f7a-f6f04956c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_mlflow(tag, model, params, metrics, registered_model_name, X_train):\n",
    "\n",
    "    # Démarrer une expérimentation dans MLflow\n",
    "    with mlflow.start_run():\n",
    "    \n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Loguer les métriques\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "            #mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            #mlflow.log_metric(\"auc\", auc)\n",
    "    \n",
    "        # Set a tag that we can use to remind ourselves what this run was for\n",
    "        mlflow.set_tag(tag[0], tag[1])\n",
    "    \n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(X_train, model.predict(X_train))\n",
    "        \n",
    "        # Loguer le modèle\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"iris_model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train,\n",
    "            registered_model_name=registered_model_name,\n",
    "        )\n",
    "\n",
    "        for key, value in metrics.items():\n",
    "            print(key, \":\" , value)\n",
    "    \n",
    "    print(f\"Modèle enregistré en version {model_info.mlflow_version}\")\n",
    "\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a26b8-6be1-4f88-88f8-f0c7a1ac9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"accuracy\": accuracy, \"auc\": auc}\n",
    "registered_model_name=\"modele_test_quickstart_2\"\n",
    "tag = \"Training Info\", \"Basic LR model for iris data\"\n",
    "\n",
    "model_info = tracking_mlflow(tag, model, params, metrics, registered_model_name, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58189916-d237-4fdf-82c8-d53a23217a7b",
   "metadata": {},
   "source": [
    "**Lister les versions du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7622021-f2a4-4603-9129-a7a7b69b51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "versions = client.get_latest_versions(\"modele_test_quickstart_2\")\n",
    "for v in versions:\n",
    "    print(f\"Version: {v.version}, Statut: {v.current_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655614e-33ec-4644-b3db-e8dc85e91f47",
   "metadata": {},
   "source": [
    "### 1.6 - Chargez le modèle en tant que fonction Python, et l'utiliser pour une prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733cdc8-1a84-478f-8499-beb80520e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the model back for predictions as a generic Python Function model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de775f74-9975-49af-8ae2-8978b3f255c3",
   "metadata": {},
   "source": [
    "### 1.7 - Lancement de l'UI\n",
    "\n",
    "Pour  la visualisation et la comparaison des expérimentations, ainsi que le stockage de manière centralisée des modèles.\n",
    "\n",
    "Cliquer sur ce lien : http://127.0.0.1:5000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90879cb-5a63-46da-aa28-787126d15176",
   "metadata": {},
   "source": [
    "## 2 - Analyser le jeu de données\n",
    "\n",
    "Je suis le kernel suivant : https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook\n",
    "\n",
    "### 2.1 - Charger les données du fichier application_train\n",
    "\n",
    "Le fichier application_test n'est pas utile, car il est destiné au concours Kaggle. Il ne contient pas la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a838fff-e1be-4cd8-8944-cd6e338cdc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('C:/Users/admin/Documents/Projets/Projet_7/data_projet/application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd93c63-d1fd-4c84-a489-54aebaa4abf7",
   "metadata": {},
   "source": [
    "The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the TARGET (the label we want to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201b623-b066-44db-866a-d2149fc67d2f",
   "metadata": {},
   "source": [
    "### 2.2 - Analyser la distribution de la variable cible TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa956f2-94a3-4a31-93d9-30c6b35eedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4a07a-7ae4-49aa-b01c-e3ecc7e31dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5c2c3-71e7-49cc-94b4-08766f1129fd",
   "metadata": {},
   "source": [
    "Il y a un déséquilibre important entre les 2 classes :\n",
    "- 0 : le prêt a été remboursé\n",
    "- 1 : le prêt n'a pas été remboursé.\n",
    "\n",
    "Il y a plus de prêts remboursés que de prêts non remboursés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc7145-19b0-44b9-88d6-b1462c42b64b",
   "metadata": {},
   "source": [
    "### 2.3 - Examiner les valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ebd9b-95ad-4409-ac7b-95e3056eba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1e3fb-2ffb-4fdf-a14e-3f9f3c1652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ee432-9983-4998-b36c-9ae1d814f3df",
   "metadata": {},
   "source": [
    "### 2.4 - Examiner les types de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631d06b-73cb-45ea-9944-e4d9ffa0002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of each type of column\n",
    "app_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35263c06-59e1-40a2-adee-ef0e25e464f6",
   "metadata": {},
   "source": [
    "Regardons le nombre de valeurs uniques dans les colonnes de type object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5453142-54ff-478e-98a3-f33c73ba9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique classes in each object column\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a29609-293e-4213-9840-6940a40330af",
   "metadata": {},
   "source": [
    "La plupart des variables catégorielles ont un nombre relativement faible d'entrées uniques. Nous devons transformer ces catégories en variable numérique via un processus d'encodage : étiquettes (label) versus one-hot.\n",
    "\n",
    "Le problème avec le codage des étiquettes est qu'il donne aux catégories un ordre arbitraire.\n",
    "\n",
    "Nous utiliserons le codage d'étiquettes (Binary encode) pour toutes les variables catégorielles avec seulement 2 catégories, et le codage one-hot (get_dummies de pandas) pour toutes les variables catégorielles avec plus de 2 catégories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71259aff-1b6f-4220-b12e-c3b6537b3056",
   "metadata": {},
   "source": [
    "### 2.6 - Recherche des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f9df6-08a3-4684-9598-97151ae10446",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73765ffa-0738-47a2-80d7-5628c54343fa",
   "metadata": {},
   "source": [
    "Les valeurs DAYS_xxx sont négatives car elles sont enregistrés par rapport à la demande de prêt en cours. Elles seront traitées dans le feature engineering.\n",
    "\n",
    "**La valeur max deLa variable DAYS_EMPLOYED contient des valeurs aberrantes :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643afe1-eddc-428d-9a32-7377b31e8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8b082-9299-49cf-aa0a-942abd5a50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b113c11-2f90-4ca8-9156-ae9b28bd679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' % len(anom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864f50a-0ab2-45ea-9607-7c18a5b26417",
   "metadata": {},
   "source": [
    "Les anomalies ont un taux de défaut plus faible.\n",
    "\n",
    "Puisque toutes les anomalies ont exactement la même valeur, nous voulons les remplir avec la même valeur au cas où tous ces prêts partageraient quelque chose en commun. En guise de solution, nous allons remplir les valeurs anormales avec un nombre (np.nan) lors du feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798e30f-8877-4f71-82f5-d962f606368a",
   "metadata": {},
   "source": [
    "### 2.7 - Corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a6218-8ef5-4a11-a9e0-c674ff7d7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlations with the target and sort\n",
    "correlations = app_train.select_dtypes(include=['number']).corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2461862-0228-4154-bdc1-4eb069b4676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation of the positive days since birth and target\n",
    "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c748e7-4251-49cb-a3cf-8bd43ed4025c",
   "metadata": {},
   "source": [
    "Il existe une relation linéaire négative avec la cible, ce qui signifie qu'à mesure que les clients vieillissent, ils ont tendance à rembourser leurs prêts à temps plus souvent.\n",
    "\n",
    "Commençons par examiner cette variable. Tout d'abord, nous pouvons créer un histogramme de l'âge. Nous mettrons l'axe des x en années pour rendre le tracé un peu plus compréhensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb854f-b476-43ca-8f82-7a93368141fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e12eb7-fad3-4830-a3e9-10e594630861",
   "metadata": {},
   "source": [
    "Pour visualiser l'effet de l'âge sur la cible, nous allons ensuite créer un graphique d'estimation de la densité du noyau (KDE) coloré par la valeur de la cible. \n",
    "\n",
    "Un graphique d'estimation de la densité du noyau montre la distribution d'une seule variable et peut être considéré comme un histogramme lissé (il est créé en calculant un noyau, généralement un gaussien, à chaque point de données, puis en faisant la moyenne de tous les noyaux individuels pour développer une seule courbe lisse). \n",
    "\n",
    "Nous utiliserons le kdeplot de Seaborn pour ce graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e15012-857d-4ea4-ab9b-f81c85dd1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n",
    "\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n",
    "\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b8d32-0ef9-4946-9caf-98f79a194656",
   "metadata": {},
   "source": [
    "La courbe cible == 1 penche vers l'extrémité la plus jeune de la fourchette. Bien que cette corrélation ne soit pas significative (coefficient de corrélation de -0,07), cette variable sera probablement utile dans un modèle d'apprentissage automatique car elle affecte la cible. Examinons cette relation d'une autre manière : le taux moyen de non-remboursement des prêts par tranche d'âge.\n",
    "\n",
    "Pour réaliser ce graphique, nous découpons d'abord la catégorie d'âge en tranches de 5 ans chacune. Ensuite, pour chaque tranche, nous calculons la valeur moyenne de la cible, qui nous indique le ratio de prêts non remboursés dans chaque catégorie d'âge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e458d02-5cd9-447d-a516-320184d396a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
    "age_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b3cb3-ea83-4747-9223-76c6d97a75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7066b01-4046-4818-85a0-efffd27b0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e600a07-431b-4eed-a18b-ea30afa3980c",
   "metadata": {},
   "source": [
    "La tendance est claire : les jeunes demandeurs sont plus susceptibles de ne pas rembourser leur prêt ! Le taux de non-remboursement est supérieur à 10 % pour les trois tranches d’âge les plus jeunes et inférieur à 5 % pour la tranche d’âge la plus âgée.\n",
    "\n",
    "Ces informations pourraient être directement utilisées par la banque : comme les jeunes clients sont moins susceptibles de rembourser leur prêt, il faudrait peut-être leur fournir davantage de conseils ou de conseils en matière de planification financière. Cela ne signifie pas que la banque doit faire preuve de discrimination à l’égard des jeunes clients, mais il serait judicieux de prendre des mesures de précaution pour les aider à payer à temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0165cb-0fee-4e5a-a2fa-959a55a8f28f",
   "metadata": {},
   "source": [
    "Sources extérieures\n",
    "\n",
    "Les 3 variables ayant les corrélations négatives les plus fortes avec la cible sont EXT_SOURCE_1, EXT_SOURCE_2 et EXT_SOURCE_3. Selon la documentation, ces caractéristiques représentent un « score normalisé provenant d'une source de données externe ». Je ne sais pas exactement ce que cela signifie, mais il peut s'agir d'une sorte de notation de crédit cumulative réalisée à l'aide de nombreuses sources de données.\n",
    "\n",
    "Examinons ces variables.\n",
    "\n",
    "Tout d'abord, nous pouvons montrer les corrélations des caractéristiques EXT_SOURCE avec la cible et entre elles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1ce8f-e81c-4b85-8c15-dcf9c715d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e6c4f-e41b-4575-8dbd-d5cd77c707c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463adc3-bdbf-4eb5-a017-0bcba020e891",
   "metadata": {},
   "source": [
    "Les trois caractéristiques EXT_SOURCE ont des corrélations négatives avec la cible, ce qui indique qu'à mesure que la valeur de EXT_SOURCE augmente, le client est plus susceptible de rembourser le prêt. Nous pouvons également voir que DAYS_BIRTH est positivement corrélé à EXT_SOURCE_1, ce qui indique que l'un des facteurs de ce score est peut-être l'âge du client.\n",
    "\n",
    "Nous pouvons ensuite examiner la distribution de chacune de ces caractéristiques colorées par la valeur de la cible. Cela nous permettra de visualiser l'effet de cette variable sur la cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf50334-066b-4e65-a93d-e3faf8ba8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ac054-b0c7-4717-a256-cc674812f59e",
   "metadata": {},
   "source": [
    "EXT_SOURCE_3 affiche la plus grande différence entre les valeurs de la cible. Nous pouvons clairement voir que cette caractéristique a une certaine relation avec la probabilité qu'un demandeur rembourse un prêt. La relation n'est pas très forte (en fait, elles sont toutes considérées comme très faibles, mais ces variables seront toujours utiles à un modèle d'apprentissage automatique pour prédire si un demandeur remboursera ou non un prêt à temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b8e0b-89aa-4a8b-b8a8-4b64ad8fe7cb",
   "metadata": {},
   "source": [
    "## 3 - Feature engineering\n",
    "\n",
    "Je m'appuie sur le Kernel : https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script\n",
    "\n",
    "On ne reprend pas les transformations de fichier applaction_test qui ne nous concerne pas ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175219fe-7835-42ee-b55b-6147e637d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "path = \"C:/Users/admin/Documents/Projets/Projet_7/data_projet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223a50d-ae62-4aa9-af13-f1470a16c51a",
   "metadata": {},
   "source": [
    "### 3.1 - Créer une fonction pour le one-hot encoding\n",
    "\n",
    "Elle prend en entrée un dataframe, et renvoie le dataframe avec les variables de type Object transformée vie one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023f263-d8f4-46d8-ac9d-6ab3a0dd0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02464a3c-5bb9-4a10-8da2-4b30a1585f5e",
   "metadata": {},
   "source": [
    "### 3.2 - Pré traiter le fichier application_train\n",
    "\n",
    "On transforme les variables catégorielles :\n",
    "- celles avec 2 valeurs en mode binaire\n",
    "- Celles avec plus que 2 valeurs avec one-hot\n",
    "\n",
    "Puis on remplace les valeurs aberrantes de la variable DAYS_EMPLOYED vu précédemment.\n",
    "\n",
    "Puis on crée de nouvelles features en pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf11db0-c75d-4e08-9b34-0008de611e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train(num_rows = None, nan_as_category = False, path = None):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv(path + 'application_train.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}\".format(len(df)))\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']        # proportion de la vie d'une personne passée en emploi\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']     # rapport entre le revenu total et le montant du crédit demandé\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS'] # revenu par membre du foyer\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']   # part du revenu utilisée pour rembourser une annuité\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']                # proportion du crédit remboursée à chaque échéance\n",
    "    #gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0281d49-ce8d-4e8e-a36f-b6a6ae6d6276",
   "metadata": {},
   "source": [
    "Testons ce pré traitement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2678a95-5e5a-4c77-a866-a4f2f51352c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows=10000, path=path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723f557-fb72-493f-9e5e-5efda7647c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79368a4d-1d19-4af5-843a-4ab7a73b5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd814c94-8947-46e4-a8db-88158a27f901",
   "metadata": {},
   "source": [
    "### 3.4 - Pré traiter les fichiers bureau et bureau_balance\n",
    "\n",
    "Créons une fonction pour pré-traiter et agrèger les fichiers bureau.csv et bureau_balance.csv pour extraire des caractéristiques utiles.\n",
    "\n",
    "- bureau.csv contient des informations sur les crédits passés des clients auprès d'autres banques.\n",
    "- bureau_balance.csv contient l'historique mensuel des statuts des crédits enregistrés dans bureau.csv.\n",
    "\n",
    "La fonction réalise les actions suivantes :\n",
    "- Encodage des variables catégorielles (One-Hot Encoding)\n",
    "- Agrégation de bureau_balance.csv par SK_ID_BUREAU\n",
    "- Agrégation de bureau.csv par SK_ID_CURR\n",
    "- Séparation entre crédits actifs et clôturés (Création des caractéristiques spécifiques aux crédits actifs et clôturés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69143e-0216-4bb2-b3de-110870b9c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True, path = None):\n",
    "    bureau = pd.read_csv(path + 'bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv(path + 'bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}  # Date la plus ancienne, la plus récente d'un enregistrement, et le nbre d'enregistrements pour un crédit donné\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    #gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    # agrège toutes les valeurs numériques de chaque client\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    # et agrège toutes les variables catégorielles en prenant la moyenne.\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "\n",
    "    # les nouvelles colonnes auront le préfix BURO\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # Séparation entre crédits actifs et clôturés\n",
    "    \n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    #gc.collect()\n",
    "    \n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    #gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36a615-a2fd-4d86-a6fd-91c7f9a01c83",
   "metadata": {},
   "source": [
    "Testons avec ce pré traitement en plus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61b2fa-e9cf-4f78-a1cc-0721626f471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows = 10000, path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows = 10000, path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3402a-807a-406d-9c1a-e4ea4019e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fce90-61f2-4c24-9b7d-bb8824a49fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2055c-339d-4f50-824c-5f23bb2c668c",
   "metadata": {},
   "source": [
    "### 3.5 - Prétraiter le fichier previous_application\n",
    "\n",
    "Créons une fonction pour prétraiter le fichier previous_application.csv, qui contient des informations sur les demandes de crédit précédentes des clients.\n",
    "\n",
    "- Encodage One-Hot pour les colonnes catégorielles\n",
    "- Remplacement des valeurs spécifiques par NaN\n",
    "- Création de nouvelles caractéristiques (comme des pourcentages)\n",
    "- Agrègation les données par SK_ID_CURR (identifiant client) à la fois pour des caractéristiques numériques et catégorielles\n",
    "- Séparation des demandes approuvées et refusées, puis les agrège séparément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bdc49-322a-4a3b-985d-1251d67de717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True, path = None):\n",
    "    prev = pd.read_csv(path + 'previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    \n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    #gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c5105-a037-44c7-a1fe-6d1aac26eac3",
   "metadata": {},
   "source": [
    "Testons avec ce pré traitement avec les précédents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949f883-9575-4c4b-b871-f6cda84811cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows = 10000, path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows = 10000, path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(num_rows = 10000, path = path)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d4f11-4e41-4699-966a-405e8dc4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b029b03-4039-442b-87cb-11d891f3e406",
   "metadata": {},
   "source": [
    "### 3.6 - Prétraiter le fichier POS_CASH_balance\n",
    "\n",
    "Créons une fonction pour rétraiter les données provenant du fichier POS_CASH_balance.csv, qui contient des informations sur l'historique des paiements effectués avec les cartes de crédit ou autres moyens de paiement utilisés par les clients.\n",
    "\n",
    "- Encodage One-Hot des colonnes catégorielles.\n",
    "- Agrégations statistiques sur des colonnes numériques spécifiques, telles que la valeur maximale, moyenne et la taille de certaines variables.\n",
    "- Agrègation des informations des colonnes catégorielles avec la moyenne.\n",
    "- Calcul du nombre de comptes POS par client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc18d4-5c52-4a9a-bb26-69aa97f8671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True, path = None):\n",
    "    pos = pd.read_csv(path + 'POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    \n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    #gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83200dda-0db8-46b5-a622-92f8d70a6c04",
   "metadata": {},
   "source": [
    "Testons avec ce pré traitement avec les précédents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592024a-6d20-4289-85da-a3eab8682a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows = 10000, path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows = 10000, path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(num_rows = 10000, path = path)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process POS-CASH balance\"):\n",
    "    pos = pos_cash(num_rows = 10000, path = path)\n",
    "    print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680443be-21f3-4ffd-b975-9429f68c6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c76cd2-4c33-4133-b181-dd53421c70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.7 - Prétraiter le fichier installments_payments\n",
    "\n",
    "Créons une fonction pour prétraiter le fichier contenant des informations sur les paiements de crédits ou de prêts par les clients.\n",
    "\n",
    "- Encodage des variables catégorielles\n",
    "- Calcul des nouvelles caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b169394-90ee-4c42-be2c-d19094fd2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True, path = path):\n",
    "    ins = pd.read_csv(path + 'installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    \n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT'] # pourcentage de paiement par rapport à l'annuité\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT'] # Différence entre le paiement effectué et l'annuité\n",
    "    \n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT'] # Jours de retard (DPD - Days Past Due) et jours avant l'échéance (DBD - Days Before Due)\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT'] \n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0) # Correction des valeurs négatives \n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0) #Correction des valeurs négatives \n",
    "    \n",
    "    # Features: Perform aggregations\n",
    "    # Agrégation des caractéristiques numériques\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "\n",
    "    # Agrégation des colonnes catégorielles\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    # Application des agrégations par client (SK_ID_CURR)\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    #gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd5012-ffe9-4b41-be49-a76eb05fcfee",
   "metadata": {},
   "source": [
    "Testons avec ce pré traitement avec les précédents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690e734-9ce3-41ac-8141-242d667900f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows = 10000, path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows = 10000, path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(num_rows = 10000, path = path)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process POS-CASH balance\"):\n",
    "    pos = pos_cash(num_rows = 10000, path = path)\n",
    "    print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process installments payments\"):\n",
    "    ins = installments_payments(num_rows = 10000, path = path)\n",
    "    print(\"Installments payments df shape:\", ins.shape)\n",
    "    df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d17f3f-bcc8-43c0-a0c4-3a372f2886ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbff6a-f40d-4b1b-82e3-9cdeffa9bbad",
   "metadata": {},
   "source": [
    "### 3.8 - Prétraiter le fichier credit_card_balance\n",
    "\n",
    "Créons une fonction pour prétraiter un fichier contenant des informations sur les cartes de crédit.\n",
    "\n",
    "- Encodage des variables catégorielles\n",
    "- Suppression de la colonne SK_ID_PREV\n",
    "- Agrégation des données par SK_ID_CURR (ID client), en calculant des statistiques comme le minimum, maximum, moyenne, somme, et variance pour chaque colonne\n",
    "- Ajout d'une colonne indiquant le nombre de cartes de crédit par client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c1aee-f807-41b4-97b5-1a8f8203e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True, path = path):\n",
    "    cc = pd.read_csv(path + 'credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    \n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    #gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c8d5-30eb-4637-bedf-b0ca9a43fbe7",
   "metadata": {},
   "source": [
    "Testons avec ce pré traitement avec les précédents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d335af-109b-4b92-8ae2-f93549d03642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(num_rows = 10000, path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(num_rows = 10000, path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(num_rows = 10000, path = path)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process POS-CASH balance\"):\n",
    "    pos = pos_cash(num_rows = 10000, path = path)\n",
    "    print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process installments payments\"):\n",
    "    ins = installments_payments(num_rows = 10000, path = path)\n",
    "    print(\"Installments payments df shape:\", ins.shape)\n",
    "    df = df.join(ins, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process credit card balance\"):\n",
    "    cc = credit_card_balance(num_rows = 10000, path = path)\n",
    "    print(\"Credit card balance df shape:\", cc.shape)\n",
    "    df = df.join(cc, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2014a88-2468-468a-8249-05e21a490428",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e6558-b7da-4f98-b41a-a55bb82bf986",
   "metadata": {},
   "source": [
    "### 3.9 - créer le dataframe complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fbf4e-8f09-4e2a-b999-ab69e0afaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = application_train(path = path)\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance(path = path)\n",
    "    print(\"Bureau df shape:\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications(path = path)\n",
    "    print(\"Previous applications df shape:\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process POS-CASH balance\"):\n",
    "    pos = pos_cash(path = path)\n",
    "    print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')   # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process installments payments\"):\n",
    "    ins = installments_payments(path = path)\n",
    "    print(\"Installments payments df shape:\", ins.shape)\n",
    "    df = df.join(ins, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "with timer(\"Process credit card balance\"):\n",
    "    cc = credit_card_balance( path = path)\n",
    "    print(\"Credit card balance df shape:\", cc.shape)\n",
    "    df = df.join(cc, how='left', on='SK_ID_CURR')  # jointure à gauche avec le dataframe principal\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ae893-5122-47a5-a6bd-9e84639768ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad9ceb8-8cb0-4ac3-bcb5-cd1fc4a4c816",
   "metadata": {},
   "source": [
    "### 3.10 - Traiter les valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629ed68-47d7-4899-9adc-97c69d7ad4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(df)\n",
    "missing_values.head(130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed42ab7-f35c-418d-8ecf-8cc1fd0df18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96707b97-7c75-46fe-86be-39ff9d3d043d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe7861-4eb2-4653-8ae2-1322d9e2010a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290e046-79b8-4f24-b46d-ad034a21d61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3a2d4-e895-4d60-b48b-d9e0033d2d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6cfb61-d708-4f64-9f96-4398dd6fceab",
   "metadata": {},
   "source": [
    "## 4 - Créer un score métier pour l'entraînement des modèles\n",
    "\n",
    "### 4.1 - Créer une fonction qui calcule le coût des erreurs de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da12d73-06ed-4575-aad7-ed468c171659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors_cost(y_true, y_pred):\n",
    "    # Calcule le coût des erreurs de prédiction : 10*FN + FP\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()  # ravael pou transformer la matrice 2x2 en 4 valeurs\n",
    "    return 10 * fn + fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dd159-4878-4052-8a0d-24cde164c6d9",
   "metadata": {},
   "source": [
    "### 4.2 - Créer le score métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59395d9c-12e1-464b-be8e-db1a62673485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du scorer métier - plus le score est petit mieux sait\n",
    "business_score = make_scorer(business_cost, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de16ba9-31c3-4ee1-97e8-f76b6e9ea286",
   "metadata": {},
   "source": [
    "### 4.3 - Créer une fonction qui trouve le seuil optimal\n",
    "\n",
    "Le seuil optimal est la valeur à partir de laquelle on décide si un client doit obtenir ou non un crédit, basé sur la probabilité prédite par le modèle. Le but est de trouver un seuil qui minimise le coût métier, c'est-à-dire le coût total des erreurs de prédiction en tenant compte des faux positifs (FP) et des faux négatifs (FN), où les faux négatifs sont 10 fois plus coûteux.\n",
    "\n",
    "Le seuil optimal est celui qui donne le coût total le plus bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de98864-3b21-4af6-b08a-93cb1d2bb291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_threshold(y_true, y_scores):\n",
    "    # y_scores est la liste des probabilités d'être en classe 1\n",
    "    # Trouve le seuil qui minimise le coût métier : 10*FN + FP\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "    # tpr : Probabilité qu'un bon client (1) soit bien détecté comme bon\n",
    "    # fpr : Probabilité qu'un mauvais client (0) soit faussement accepté comme bon\n",
    "\n",
    "    # Calcul du coût métier pour chaque seuil =\n",
    "    # Coûts des faux négatifs 10 * (1 - tpr[i]) * sum(y_true)\n",
    "    # + Coûts total des faux positifs fpr[i] * (len(y_true) - sum(y_true))\n",
    "    costs = [10 * (1 - tpr[i]) * sum(y_true) + fpr[i] * (len(y_true) - sum(y_true))\n",
    "         for i in range(len(thresholds))]\n",
    "\n",
    "    return thresholds[np.argmin(costs)]  # Retourne le seuil optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9b002-5d76-45e2-83b6-149f1d150e75",
   "metadata": {},
   "source": [
    "### 4.4 - Créer le score \"seuil métier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01644dc0-ce12-4397-b11c-1de92d955bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_threshold_score(estimator, X, y):\n",
    "    # Calcule le seuil optimal pour minimiser le coût métier\n",
    "    y_scores = estimator.predict_proba(X)[:, 1]  # Probabilité d'être en classe 1\n",
    "    return optimal_threshold(y, y_scores)  # Retourne le seuil optimal\n",
    "\n",
    "# Création du scorer\n",
    "scorer_threshold = make_scorer(business_threshold_score, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fb99b-4cff-48a8-94b3-d4f68f95c35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c8ff1-b0b6-47fd-83bf-bf384f1c458a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59650d6-d45c-4b74-9dff-562562f6a9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65af53-b57a-45a8-8feb-85649811a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf53cb-a316-446f-87cf-ae19ba4776c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be27c0e-7c8a-423b-9685-ae23d9e8ce75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbec84-e337-42d4-abdb-3b2953a39621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0090d-3691-4382-bcf8-f2f15467a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyClassifier, Regression Logistique, Random Forest, Decision Tree, XGBoost, Lightgbm\n",
    "\n",
    "Courbe Roc, AUC, f1-score, precision, recall, time\n",
    "\n",
    "matrice de confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
